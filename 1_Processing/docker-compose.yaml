version: '3'

networks:
  pipeline_network:
    driver: bridge
    ipam:
      driver: default
      config:
          - subnet: "172.18.0.0/16"
            gateway: 172.18.0.1


services:
  spark:
    image: docker.io/bitnami/spark:3.3.0
    container_name: spark_master
    hostname: spark_master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
    networks:
      - pipeline_network

  zookeeper:
    platform: linux/amd64
    image: confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: zookeeper
    restart: unless-stopped
    ports:
      - '32181:32181'
      - '2888:2888'
      - '3888:3888'
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    healthcheck:
      test: echo stat | nc localhost 32181
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-ui: 
    container_name: kafka-ui 
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8087:8080
    depends_on:
      broker-1:
        condition: service_started
      broker-2:
        condition: service_started
      broker-3:
        condition: service_started
    environment:
      KAFKA_CLUSTERS_0_NAME: broker-1
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:29091
      KAFKA_CLUSTERS_0_METRICS_PORT: 19101
      KAFKA_CLUSTERS_1_NAME: broker-2
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker-2:29092
      KAFKA_CLUSTERS_1_METRICS_PORT: 19102
      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry0:8085
      KAFKA_CLUSTERS_2_NAME: broker-3
      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker-3:29093
      KAFKA_CLUSTERS_2_METRICS_PORT: 19103
      #KAFKA_CLUSTERS_1_SCHEMAREGISTRY: http://schemaregistry1:8085
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-1:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-1
    hostname: broker-1
    restart: unless-stopped
    ports:
      - '9091:9091'
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29091,EXTERNAL://localhost:9091
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19101
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9091
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-2:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-2
    hostname: broker-2
    restart: unless-stopped
    ports:
      - '9092:9092'
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29092,EXTERNAL://localhost:9092
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19102
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9092
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-3:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-
    hostname: broker-3
    restart: unless-stopped
    ports:
      - '9093:9093'
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29093,EXTERNAL://localhost:9093
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 19103
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9093
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"


  schema-registry:
    image: confluentinc/cp-schema-registry:7.0.1
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      broker-1:
        condition: service_started
      broker-2:
        condition: service_started
      broker-3:
        condition: service_started
    healthcheck:
      test: curl --output /dev/null --silent --head --fail http://schema-registry:8081/subjects
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker-1:29091,broker-2:29092,broker-3:29093'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    networks:
      - pipeline_network


  rest-proxy:
    image: confluentinc/cp-kafka-rest:7.4.0
    depends_on:
      broker-1:
        condition: service_started
      broker-2:
        condition: service_started
      broker-3:
        condition: service_started
      schema-registry: 
        condition: service_started
    healthcheck:
      test: curl --output /dev/null --silent --head --fail http://rest-proxy:8082
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "8082:8082"
    hostname: rest-proxy
    container_name: rest-proxy
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker-1:29091,broker-2:29092,broker-3:29093'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'      
    networks:
      - pipeline_network


  producer:
    platform: linux/amd64
    build: 
     context: ./kafka
     dockerfile: producer/Dockerfile
    container_name: producer  
    restart: always
    depends_on:
      cassandra-create-ks-topic:
        condition: service_completed_successfully
      postgres:
        condition: service_started
    command: ["sh", "/src/run.sh", "$ACTION"]  
    volumes:
      - ./kafka/producer:/src
    environment:
      ACTION: producer
      BOOTSTRAP_SERVERS: 'broker-1:29091,broker-2:29092,broker-3:29093'
      PYTHONUNBUFFERED: 1
      TOPICS: 'actor,category,film,film_actor,film_category,inventory,rental'
      stream_data_path: '/src/data/stream_data'
      batch_size: 8
      KAFKA_VERSION: '7,4,0'
    networks:
      - pipeline_network
    logging:
      driver: "json-file"
      options:
        max-size: "1m"


  cassandra:
    image: cassandra:4
    container_name: cassandra
    hostname: cassandra
    ports:
      - '9042:9042'
    volumes:
      - cassandra-data:/var/lib/cassandra
    networks:
      - pipeline_network


  cassandra-create-ks-topic:
    image: cassandra:4
    container_name: cassandra-create-ks-topic
    networks:
      - pipeline_network
    depends_on:
      cassandra:
        condition: service_started
    restart: "no"
    entrypoint: ["/cassandra-init.sh"]
    volumes:
      - ./cassandra/cassandra-init.sh:/cassandra-init.sh      


  mysql:
    image: mysql:8.0
    container_name: mysql
    hostname: mysql
    restart: always
    ports:
      - '3306:3306'
    command: --init-file /data/application/init.sql
    volumes:
        - ./mysql/init.sql:/data/application/init.sql
    environment:
      MYSQL_ROOT_USER: root
      MYSQL_ROOT_PASSWORD: passwd
      # MYSQL_USER: user1
      # MYSQL_PASSWORD: password
    networks:
      - pipeline_network   

  postgres:
    container_name: postgres
    build:
      context: ./pg/
    hostname: postgres
    environment:
        POSTGRES_DB: dvd_rentals_lake_db
        POSTGRES_USER: user
        POSTGRES_PASSWORD: secret
    ports:
      - "5432:5432"
    restart: always
    networks:
      - pipeline_network   

  consumer:
    platform: linux/amd64
    build: 
     context: ./spark
     dockerfile: consumer/Dockerfile
    container_name: consumer
    restart: always
    environment:
      - ACTION=consumer
      - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
      - PYTHONUNBUFFERED=1
      - TOPICS=actor,category,film,film_actor,film_category,inventory,rental
      - delta_table_path='/src/data/delta_table'
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=4G
      - SPARK_EXECUTOR_MEMORY=4G
      - SPARK_WORKER_CORES=4
    command: ["/bin/bash", "-c", "/spark-setup.sh"]  
    volumes:      
      - type: bind
        source: ./spark/consumer/spark-setup.sh
        target: /spark-setup.sh
      - ./spark/consumer:/home/
      - ./spark/consumer:/src
    networks:
      - pipeline_network
    depends_on:
      cassandra-create-ks-topic:
        condition: service_completed_successfully
      postgres:
        condition: service_started
      spark:
        condition: service_started
    logging:
      driver: "json-file"
      options:
        max-size: "1m"


volumes:
  my-db:
  cassandra-data: